import{_ as r,c as e,o as n,ah as t,bp as o,bq as i}from"./chunks/framework.B6i8j-aH.js";const m=JSON.parse('{"title":"학습 내용 : AI 심화 과정 (2족 보행 로봇)","description":"","frontmatter":{},"headers":[],"relativePath":"rl-adv/index.md","filePath":"rl-adv/index.md"}'),s={name:"rl-adv/index.md"};function l(d,a,p,g,h,c){return n(),e("div",null,[...a[0]||(a[0]=[t('<h1 id="학습-내용-ai-심화-과정-2족-보행-로봇" tabindex="-1">학습 내용 : AI 심화 과정 (2족 보행 로봇) <a class="header-anchor" href="#학습-내용-ai-심화-과정-2족-보행-로봇" aria-label="Permalink to “학습 내용 : AI 심화 과정 (2족 보행 로봇)”">​</a></h1><h2 id="이족-보행-강화-학습-방법" tabindex="-1">이족 보행 강화 학습 방법 <a class="header-anchor" href="#이족-보행-강화-학습-방법" aria-label="Permalink to “이족 보행 강화 학습 방법”">​</a></h2><p><img src="'+o+'"></p><p><strong>이족 보행 강화 학습(Bipedal Walking Reinforcement Learning)</strong>은 로봇에게 &quot;왼쪽 다리를 30도 들어라&quot;라고 일일이 명령하는 대신, <strong>로봇이 스스로 넘어지면서 걷는 법을 터득하게 만드는 인공지능 기술</strong>입니다.</p><p>쉽게 비유하면 <strong>&#39;아기가 걸음마를 배우는 과정&#39;</strong>과 똑같습니다.</p><p>핵심 원리를 3단계로 요약하면 다음과 같습니다.</p><h3 id="_1-시도-action" tabindex="-1">1. 시도 (Action) <a class="header-anchor" href="#_1-시도-action" aria-label="Permalink to “1. 시도 (Action)”">​</a></h3><p>로봇(에이전트)이 무작위로 다리 관절을 움직여 봅니다. 처음에는 제어할 줄 몰라 바로 쓰러집니다.</p><h3 id="_2-보상과-벌칙-reward-penalty" tabindex="-1">2. 보상과 벌칙 (Reward &amp; Penalty) <a class="header-anchor" href="#_2-보상과-벌칙-reward-penalty" aria-label="Permalink to “2. 보상과 벌칙 (Reward &amp; Penalty)”">​</a></h3><ul><li><strong>보상(+):</strong> 로봇이 균형을 잡거나 앞으로 조금이라도 전진하면 점수를 줍니다.</li><li><strong>벌칙(-):</strong> 로봇이 넘어지거나 비틀거리면 점수를 깎습니다.</li></ul><h3 id="_3-학습-learning" tabindex="-1">3. 학습 (Learning) <a class="header-anchor" href="#_3-학습-learning" aria-label="Permalink to “3. 학습 (Learning)”">​</a></h3><p>로봇은 <strong>&#39;점수를 가장 많이 받는 방법&#39;</strong>을 찾기 위해 수백만 번의 시뮬레이션을 반복합니다. 이 과정에서 &quot;어떤 자세를 취해야 안 넘어지는지&quot;, &quot;어떻게 힘을 줘야 앞으로 나가는지&quot;를 스스로 깨닫게 됩니다.</p><hr><p><strong>왜 이 방식을 쓰나요?</strong> 이족 보행은 균형을 잡기 매우 어려워서 사람이 일일이 수식으로 계산해서 제어하기 힘듭니다. 하지만 강화 학습을 쓰면 로봇이 울퉁불퉁한 길이나 계단 같은 험한 환경에서도 <strong>동물처럼 유연하게 걷는 법</strong>을 배울 수 있기 때문입니다.</p><h2 id="움직임-모방-학습-방법" tabindex="-1">움직임 모방 학습 방법 <a class="header-anchor" href="#움직임-모방-학습-방법" aria-label="Permalink to “움직임 모방 학습 방법”">​</a></h2><p><img src="'+i+'"></p><p><strong>이족 보행 로봇의 움직임 모방 학습(Motion Imitation Learning)</strong>은 로봇에게 <strong>&quot;이 사람(또는 동물)의 동작과 똑같이 움직여!&quot;</strong>라고 목표를 정해주는 강화 학습 방식입니다.</p><p>앞서 설명한 일반적인 강화 학습이 &#39;혼자 뒹굴며 걷는 법을 깨우치는 것&#39;이라면, 모방 학습은 <strong>&#39;댄스 강사의 춤을 보고 그대로 따라 추는 것&#39;</strong>에 비유할 수 있습니다.</p><p>핵심 과정은 다음과 같습니다.</p><h3 id="_1-정답지-제공-reference-motion" tabindex="-1">1. 정답지 제공 (Reference Motion) <a class="header-anchor" href="#_1-정답지-제공-reference-motion" aria-label="Permalink to “1. 정답지 제공 (Reference Motion)”">​</a></h3><p>사람이 걷거나 뛰는 실제 움직임을 녹화한 데이터(<strong>모션 캡처</strong>)를 로봇에게 &#39;정답지&#39;로 줍니다.</p><h3 id="_2-따라-하기와-채점-tracking-reward" tabindex="-1">2. 따라 하기와 채점 (Tracking &amp; Reward) <a class="header-anchor" href="#_2-따라-하기와-채점-tracking-reward" aria-label="Permalink to “2. 따라 하기와 채점 (Tracking &amp; Reward)”">​</a></h3><p>로봇은 이 동작을 흉내 냅니다. 이때 인공지능은 두 가지를 동시에 평가합니다.</p><ul><li><strong>유사성 보상:</strong> &quot;관절 각도가 사람과 얼마나 똑같은가?&quot; (똑같을수록 점수 ↑)</li><li><strong>물리적 균형:</strong> &quot;따라 하면서 넘어지지는 않았는가?&quot;</li></ul><h3 id="_3-자연스러운-동작-완성" tabindex="-1">3. 자연스러운 동작 완성 <a class="header-anchor" href="#_3-자연스러운-동작-완성" aria-label="Permalink to “3. 자연스러운 동작 완성”">​</a></h3><p>처음에는 흉내 내기에 급급해 자주 넘어지지만, 반복 학습을 통해 <strong>균형을 잡으면서도 사람처럼 자연스럽게 걷거나 뛰는 법</strong>을 익히게 됩니다.</p><hr><p><strong>왜 이 방식을 쓰나요?</strong> 일반적인 강화 학습만으로 로봇을 훈련시키면, 로봇이 넘어지지만 않으려고 팔을 마구 흔들거나 <strong>좀비처럼 기괴하게 걷는 경우</strong>가 많습니다. 모방 학습을 사용하면 <strong>사람처럼 우아하고 자연스러운 움직임</strong>을 만들 수 있습니다.</p>',28)])])}const f=r(s,[["render",l]]);export{m as __pageData,f as default};
